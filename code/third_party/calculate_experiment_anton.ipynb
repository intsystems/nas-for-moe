{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d1f4f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "from math import log2\n",
    "from typing import List, Tuple, Dict\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "467bd3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cifar_dataloader(\n",
    "    train_batch_size: int = 256, \n",
    "    test_batch_size: int = 2048, \n",
    "    device: torch.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    ") -> Tuple[DataLoader, DataLoader]:\n",
    "    \"\"\"\n",
    "    Loads CIFAR-10 and returns DataLoader for training and testing (3-channel RGB).\n",
    "    \n",
    "    Args:\n",
    "        train_batch_size: Batch size for training data\n",
    "        test_batch_size: Batch size for test data\n",
    "        device: Target device ('cuda' or 'cpu')\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (train_loader, test_loader)\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor()  # Converts to tensor and normalizes to [0,1]\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.CIFAR10(\n",
    "        root='./data', \n",
    "        train=True, \n",
    "        transform=transform, \n",
    "        download=True\n",
    "    )\n",
    "    test_dataset = datasets.CIFAR10(\n",
    "        root='./data', \n",
    "        train=False, \n",
    "        transform=transform, \n",
    "        download=True\n",
    "    )\n",
    "\n",
    "    # Convert to tensors (N, C, H, W) format\n",
    "    train_data = torch.stack([x for x, _ in train_dataset], dim=0)\n",
    "    test_data = torch.stack([x for x, _ in test_dataset], dim=0)\n",
    "\n",
    "    if device.type == 'cuda':\n",
    "        train_data = train_data.to(device)\n",
    "        test_data = test_data.to(device)\n",
    "\n",
    "    train_dataset = TensorDataset(train_data)\n",
    "    test_dataset = TensorDataset(test_data)\n",
    "\n",
    "    if device.type.startswith(\"cuda\"):\n",
    "        train_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False)\n",
    "    else:\n",
    "        train_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False, num_workers=8, pin_memory=True)\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca3a4fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fashion_mnist_dataloader(\n",
    "    train_batch_size: int = 256, \n",
    "    test_batch_size: int = 2048, \n",
    "    device: torch.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    ") -> Tuple[DataLoader, DataLoader]:\n",
    "    \"\"\"\n",
    "    Loads Fashion-MNIST and returns DataLoader for training and testing (1-channel grayscale).\n",
    "    \n",
    "    Args:\n",
    "        train_batch_size: Batch size for training data\n",
    "        test_batch_size: Batch size for test data\n",
    "        device: Target device ('cuda' or 'cpu')\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (train_loader, test_loader)\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor()  # Converts to tensor and normalizes to [0,1]\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.FashionMNIST(\n",
    "        root='./data', \n",
    "        train=True, \n",
    "        transform=transform, \n",
    "        download=True\n",
    "    )\n",
    "    test_dataset = datasets.FashionMNIST(\n",
    "        root='./data', \n",
    "        train=False, \n",
    "        transform=transform, \n",
    "        download=True\n",
    "    )\n",
    "\n",
    "    # Convert to tensors (N, C, H, W) format\n",
    "    train_data = torch.stack([x for x, _ in train_dataset], dim=0)\n",
    "    test_data = torch.stack([x for x, _ in test_dataset], dim=0)\n",
    "\n",
    "    if device.type == 'cuda':\n",
    "        train_data = train_data.to(device)\n",
    "        test_data = test_data.to(device)\n",
    "\n",
    "    train_dataset = TensorDataset(train_data)\n",
    "    test_dataset = TensorDataset(test_data)\n",
    "\n",
    "    if device.type.startswith(\"cuda\"):\n",
    "        train_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False)\n",
    "    else:\n",
    "        train_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False, num_workers=8, pin_memory=True)\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d22ccbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    \"\"\"Variational Autoencoder for CIFAR-10 (3-channel input).\"\"\"\n",
    "    \n",
    "    def __init__(self, input_channels: int = 3, input_size: int = 32, \n",
    "                 filter_sizes: List[int] = [32, 64, 128], latent_dim: int = 64, kernel_size: int = 3,\n",
    "                 stride: int = 2, padding: int = 1, is_variational: bool = True):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.input_channels = input_channels\n",
    "        self.input_size = input_size\n",
    "        self.filter_sizes = filter_sizes\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.is_variational = is_variational\n",
    "        \n",
    "        self._build_encoder()\n",
    "        self._calculate_conv_output_shape()\n",
    "        self._build_latent_layers()\n",
    "        self._build_decoder()\n",
    "\n",
    "    def _build_encoder(self) -> None:\n",
    "        encoder_layers = []\n",
    "        in_channels = self.input_channels\n",
    "        \n",
    "        for fs in self.filter_sizes:\n",
    "            encoder_layers.extend([\n",
    "                nn.Conv2d(in_channels, fs, kernel_size=self.kernel_size, stride=self.stride, padding=self.padding),\n",
    "                nn.BatchNorm2d(fs),\n",
    "                nn.ReLU()\n",
    "            ])\n",
    "            in_channels = fs\n",
    "            \n",
    "        self.encoder = nn.Sequential(*encoder_layers)\n",
    "\n",
    "    def _calculate_conv_output_shape(self) -> None:\n",
    "        \"\"\"\n",
    "        Вычисляет размер изображения после применения `num_convs` свёрточных слоёв (Conv2d).\n",
    "\n",
    "        Args:\n",
    "            input_size (int): Размер входа (N для изображения N x N).\n",
    "            kernel_size (int): Размер ядра свёртки (K).\n",
    "            stride (int): Шаг свёртки (S).\n",
    "            padding (int): Паддинг (P).\n",
    "            num_convs (int): Количество свёрточных слоёв (M).\n",
    "\n",
    "        Returns:\n",
    "            int: Размер изображения после M свёрток.\n",
    "        \"\"\"\n",
    "        current_size = self.input_size\n",
    "        self.data_sizes = [current_size]\n",
    "        for _ in self.filter_sizes:\n",
    "            current_size = int(np.floor((current_size + 2 * self.padding - self.kernel_size) / self.stride + 1))\n",
    "            self.data_sizes.append(current_size)\n",
    "\n",
    "        self.flat_dim = self.filter_sizes[-1] * current_size * current_size\n",
    "        self.conv_out_shape = torch.Size([int(self.filter_sizes[-1]), current_size, current_size]) # (C, H, W)\n",
    "\n",
    "    def _build_latent_layers(self) -> None:\n",
    "        self.fc_mu = nn.Linear(self.flat_dim, self.latent_dim)\n",
    "        self.fc_logvar = nn.Linear(self.flat_dim, self.latent_dim)\n",
    "        self.fc_decode = nn.Linear(self.latent_dim, self.flat_dim)\n",
    "\n",
    "    def _build_decoder(self) -> None:\n",
    "        decoder_layers = []\n",
    "        reversed_filters = list(reversed(self.filter_sizes))\n",
    "        in_channels = reversed_filters[0]\n",
    "        \n",
    "        for i, fs in enumerate(reversed_filters[1:] + [self.input_channels]):\n",
    "            decoder_layers.append(\n",
    "                nn.ConvTranspose2d(\n",
    "                    in_channels, \n",
    "                    fs, \n",
    "                    kernel_size=self.kernel_size, \n",
    "                    stride=self.stride, \n",
    "                    padding=self.padding,\n",
    "                    output_padding=self.data_sizes[::-1][i+1] - ((self.data_sizes[::-1][i] - 1) * self.stride - 2 * self.padding + self.kernel_size)\n",
    "                )\n",
    "            )\n",
    "            if i < len(reversed_filters) - 1:  # No BN/ReLU on last layer\n",
    "                decoder_layers.append(nn.BatchNorm2d(fs))\n",
    "                decoder_layers.append(nn.ReLU())\n",
    "            in_channels = fs\n",
    "            \n",
    "        decoder_layers.append(nn.Sigmoid())  # Final activation\n",
    "        self.decoder = nn.Sequential(*decoder_layers)\n",
    "\n",
    "    def encode(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        x = self.encoder(x)\n",
    "        # print(\"x = self.encoder(x)\", x.size())\n",
    "        x_flat = x.view(x.size(0), -1)\n",
    "        # print(\"x_flat = x.view(x.size(0), -1)\", x_flat.size())\n",
    "        self.encoder_exit = x_flat # Нужно для вычисления mi\n",
    "        return self.fc_mu(x_flat), self.fc_logvar(x_flat)\n",
    "\n",
    "    def reparameterize(self, mu: torch.Tensor, logvar: torch.Tensor) -> torch.Tensor:\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        if self.is_variational:\n",
    "            return mu + eps * std\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def decode(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        # print(\"z\", z.size())\n",
    "        x = self.fc_decode(z)\n",
    "        # print(\"x = self.fc_decode(z)\", x.size())\n",
    "        self.decoder_input = x # Нужно для вычисления mi\n",
    "        x = x.view(z.size(0), *self.conv_out_shape)\n",
    "        # print(\"x = x.view(z.size(0), *self.conv_out_shape)\", x.size())\n",
    "        return self.decoder(x)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "def vae_loss(recon_x: torch.Tensor, x: torch.Tensor, \n",
    "             mu: torch.Tensor, logvar: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Compute VAE loss (MSE reconstruction + KL divergence).\"\"\"\n",
    "    recon_loss = F.mse_loss(recon_x, x, reduction='sum')\n",
    "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    batch_size = x.size(0)\n",
    "    return (recon_loss + kl_loss)/batch_size, recon_loss/batch_size, kl_loss/batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "091d10ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_model_stats(vae_model: VAE) -> dict:\n",
    "    \"\"\"Calculate all model statistics (receptive field, compression ratio, etc.).\"\"\"\n",
    "    stats = {}\n",
    "\n",
    "    # 1. Basic model info\n",
    "    stats[\"latent_dim\"] = vae_model.latent_dim\n",
    "    stats[\"input_shape\"] = [vae_model.input_channels, vae_model.input_size, vae_model.input_size]\n",
    "    stats[\"filter_sizes\"] = vae_model.filter_sizes\n",
    "    stats[\"n_conv_blocks\"] = len(vae_model.filter_sizes)\n",
    "\n",
    "    # 2. Encoder/Decoder parameters\n",
    "    total_params = sum(p.numel() for p in vae_model.parameters())\n",
    "    encoder_params = sum(p.numel() for p in vae_model.encoder.parameters())\n",
    "    decoder_params = sum(p.numel() for p in vae_model.decoder.parameters())\n",
    "    stats[\"total_params\"] = total_params\n",
    "    stats[\"encoder_params\"] = encoder_params\n",
    "    stats[\"decoder_params\"] = decoder_params\n",
    "    stats[\"encoder_decoder_ratio\"] = encoder_params / decoder_params\n",
    "\n",
    "    # 3. Channel statistics\n",
    "    stats[\"max_channels\"] = max(vae_model.filter_sizes)\n",
    "    stats[\"min_channels\"] = min(vae_model.filter_sizes)\n",
    "    ## ЕСЛИ ЗАРАБОТАЕТ, ТО ПОТОМ ПЕРЕПИСАТЬ НОРМАЛЬНО!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    if len(vae_model.filter_sizes) > 1:\n",
    "        stats[\"channel_growth_factor\"] = stats[\"max_channels\"] / stats[\"min_channels\"]\n",
    "    else:\n",
    "        stats[\"channel_growth_factor\"] = 1.0\n",
    "\n",
    "    # 4. Compression ratio (spatial + channel)\n",
    "    encoder_output_shape = vae_model.conv_out_shape  # (C, H, W)\n",
    "    input_pixels = vae_model.input_size * vae_model.input_size * vae_model.input_channels\n",
    "    compressed_pixels = encoder_output_shape[0] * encoder_output_shape[1] * encoder_output_shape[2]\n",
    "    stats[\"compression_ratio\"] = input_pixels / compressed_pixels\n",
    "    stats[\"encoder_output_shape\"] = list(encoder_output_shape)\n",
    "\n",
    "    # 5. Receptive field calculation (for encoder)\n",
    "    stats[\"encoder_receptive_field\"] = calculate_receptive_field(vae_model.encoder)\n",
    "    stats[\"kernel_size\"] = vae_model.kernel_size\n",
    "    stats[\"stride\"] = vae_model.stride\n",
    "    stats[\"padding\"] = vae_model.padding\n",
    "    \n",
    "    return stats\n",
    "\n",
    "\n",
    "def calculate_receptive_field(encoder: nn.Sequential) -> int:\n",
    "    \"\"\"Calculate the receptive field of the encoder's last layer.\"\"\"\n",
    "    rf = 1\n",
    "    stride_product = 1\n",
    "    for layer in encoder:\n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            kernel_size = layer.kernel_size[0]\n",
    "            stride = layer.stride[0]\n",
    "            padding = layer.padding[0]\n",
    "            rf = rf + (kernel_size - 1) * stride_product\n",
    "            stride_product *= stride\n",
    "    return rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7ffd8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "from numpy import log\n",
    "from scipy.special import digamma\n",
    "from sklearn.neighbors import BallTree, KDTree\n",
    "\n",
    "\n",
    "# Continuous Estimators\n",
    "def entropy(x, k=3, base=2):\n",
    "    \"\"\" The classic K-L k-nearest neighbor continuous entropy estimator\n",
    "        x should be a list of vectors, e.g. x = [[1.3], [3.7], [5.1], [2.4]]\n",
    "        if x is a one-dimensional scalar and we have four samples\n",
    "    \"\"\"\n",
    "    assert k <= len(x) - 1, \"Set k smaller than num. samples - 1\"\n",
    "    x = np.asarray(x)\n",
    "    n_elements, n_features = x.shape\n",
    "    x = add_noise(x)\n",
    "    tree = build_tree(x)\n",
    "    nn = query_neighbors(tree, x, k)\n",
    "    const = digamma(n_elements) - digamma(k) + n_features * log(2)\n",
    "    return (const + n_features * np.log(nn).mean()) / log(base)\n",
    "\n",
    "def mi(x, y, z=None, k=3, base=2):\n",
    "    \"\"\" Mutual information of x and y (conditioned on z if z is not None)\n",
    "        x, y should be a list of vectors, e.g. x = [[1.3], [3.7], [5.1], [2.4]]\n",
    "        if x is a one-dimensional scalar and we have four samples\n",
    "    \"\"\"\n",
    "    assert len(x) == len(y), \"Arrays should have same length\"\n",
    "    assert k <= len(x) - 1, \"Set k smaller than num. samples - 1\"\n",
    "    x, y = np.asarray(x), np.asarray(y)\n",
    "    x, y = x.reshape(x.shape[0], -1), y.reshape(y.shape[0], -1)\n",
    "    x = add_noise(x) #add noise to both of x and y\n",
    "    y = add_noise(y)\n",
    "    points = [x, y]\n",
    "    if z is not None:\n",
    "        z = np.asarray(z)\n",
    "        z = z.reshape(z.shape[0], -1)\n",
    "        points.append(z)\n",
    "    points = np.hstack(points)\n",
    "    # Find nearest neighbors in joint space, p=inf means max-norm\n",
    "    tree = build_tree(points)\n",
    "    dvec = query_neighbors(tree, points, k)\n",
    "    if z is None:\n",
    "        a, b, c, d = avgdigamma(x, dvec), avgdigamma(\n",
    "            y, dvec), digamma(k), digamma(len(x))\n",
    "    else:\n",
    "        xz = np.c_[x, z]\n",
    "        yz = np.c_[y, z]\n",
    "        a, b, c, d = avgdigamma(xz, dvec), avgdigamma(\n",
    "            yz, dvec), avgdigamma(z, dvec), digamma(k)\n",
    "    return (-a - b + c + d) / log(base)\n",
    "\n",
    "\n",
    "# Discrete Estimators\n",
    "\n",
    "def add_noise(x, intens=1e-10):\n",
    "    # small noise to break degeneracy, see doc.\n",
    "    return x + intens * np.random.random_sample(x.shape)\n",
    "\n",
    "\n",
    "def query_neighbors(tree, x, k):\n",
    "    return tree.query(x, k=k + 1)[0][:, k]\n",
    "\n",
    "\n",
    "def count_neighbors(tree, x, r):\n",
    "    return tree.query_radius(x, r, count_only=True)\n",
    "\n",
    "\n",
    "def avgdigamma(points, dvec):\n",
    "    # This part finds number of neighbors in some radius in the marginal space\n",
    "    # returns expectation value of <psi(nx)>\n",
    "    tree = build_tree(points)\n",
    "    dvec = dvec - 1e-15\n",
    "    num_points = count_neighbors(tree, points, dvec)\n",
    "    return np.mean(digamma(num_points))\n",
    "\n",
    "\n",
    "def build_tree(points):\n",
    "    if points.shape[1] >= 20:\n",
    "        return BallTree(points, metric='chebyshev')\n",
    "    return KDTree(points, metric='chebyshev')\n",
    "\n",
    "def get_unique_probs(x):\n",
    "    uniqueids = np.ascontiguousarray(x).view(np.dtype((np.void, x.dtype.itemsize * x.shape[1])))\n",
    "    _, unique_inverse, unique_counts = np.unique(uniqueids, return_index=False, return_inverse=True, return_counts=True)\n",
    "    return np.asarray(unique_counts / float(sum(unique_counts))), unique_inverse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb9817f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import string\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy.stats import kurtosis\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cross_decomposition import CCA\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from typing import Dict\n",
    "from scipy.stats import kurtosis\n",
    "\n",
    "def generate_random_hash(length=16):\n",
    "    \"\"\"Генерирует случайный хэш из цифр и букв (A-Z, a-z)\"\"\"\n",
    "    characters = string.ascii_letters + string.digits  # Все буквы и цифры\n",
    "    return ''.join(random.choice(characters) for _ in range(length))\n",
    "\n",
    "def canonical_correlation(A, B):\n",
    "    \"\"\"Возвращает максимальную каноническую корреляцию между A и B.\"\"\"\n",
    "    assert A.shape[0] == B.shape[0]\n",
    "    cca = CCA(n_components=1)\n",
    "    cca.fit(A, B)\n",
    "    U, V = cca.transform(A, B)\n",
    "    return np.corrcoef(U.T, V.T)[0, 1]\n",
    "\n",
    "\n",
    "def train_and_evaluate(\n",
    "    config_id: int,\n",
    "    output_dir: str,\n",
    "    train_loader: DataLoader,\n",
    "    test_loader: DataLoader,\n",
    "    latent_dim: int,\n",
    "    enc_filters: int,\n",
    "    device: torch.device,\n",
    "    experement_hash: str,\n",
    "    is_variational=True,\n",
    "    kernel_size: int = 3,\n",
    "    stride: int = 2,\n",
    "    padding: int = 1,\n",
    "    epochs: int = 10,\n",
    "    input_channels: int = 3,\n",
    "    input_size: int = 32,\n",
    "    checkpoint_epochs: List[int] = [1, 5]\n",
    ") -> Dict:\n",
    "    \"\"\"Train and evaluate with metrics saving at specified epochs.\"\"\"\n",
    "    \n",
    "    print(f\"\\nConfig {config_id}: latent_dim={latent_dim}, filters={enc_filters}, kernel_size={kernel_size}, stride={stride}, padding={padding}\")\n",
    "    \n",
    "    model = VAE(\n",
    "        input_channels=input_channels,\n",
    "        input_size=input_size,\n",
    "        filter_sizes=enc_filters,\n",
    "        latent_dim=latent_dim,\n",
    "        kernel_size = kernel_size,\n",
    "        stride = stride,\n",
    "        padding = padding,\n",
    "        is_variational = is_variational\n",
    "    ).to(device)\n",
    "    \n",
    "    # Initialize optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=2e-4, weight_decay=1e-5)\n",
    "    \n",
    "    # Results storage\n",
    "    results = {\n",
    "        'config': calculate_model_stats(model), \n",
    "        'checkpoints': {}\n",
    "    }\n",
    "    \n",
    "    prev_calc_time = []\n",
    "    for epoch in tqdm(range(1, epochs + 1)):\n",
    "        start_time = time.time()\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        epoch_rec = 0.0\n",
    "        epoch_kl = 0.0\n",
    "        \n",
    "        for x, in train_loader:\n",
    "            x = x.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            recon, mu, logvar = model(x)\n",
    "            loss, rec_loss, kl_loss = vae_loss(recon, x, mu, logvar)\n",
    "\n",
    "            if recon.isnan().any():\n",
    "                print(\"Модель не обучилась на поданной конфигурации\")\n",
    "                raise ValueError(\"Модель не обучилась на поданной конфигурации\")\n",
    "            \n",
    "            if is_variational:\n",
    "                loss.backward()\n",
    "            else:\n",
    "                rec_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            epoch_rec += rec_loss.item()\n",
    "            epoch_kl += kl_loss.item()\n",
    "        \n",
    "        prev_calc_time.append(time.time() - start_time)\n",
    "        if len(prev_calc_time) >= 3:\n",
    "            if np.mean(prev_calc_time[-3:]) > 30 * 60:\n",
    "                print(\"Рассчет эпохи слишком длительный, чтобы можно было использовать его в эксперименте!\")\n",
    "                raise TimeoutError(\"Рассчет эпохи слишком длительный, чтобы можно было использовать его в эксперименте!\")\n",
    "        # Save checkpoint if epoch is in checkpoint_epochs\n",
    "        if epoch in checkpoint_epochs:\n",
    "            test_metrics = evaluate_model(model, test_loader, device)\n",
    "            results['checkpoints'][epoch] = {\n",
    "                'train_loss': epoch_loss / len(train_loader),\n",
    "                'train_rec': epoch_rec / len(train_loader),\n",
    "                'train_kl': epoch_kl / len(train_loader),\n",
    "                **test_metrics,\n",
    "                'model_state': model.state_dict()\n",
    "            }\n",
    "    \n",
    "    # Final evaluation\n",
    "    if epoch not in results['checkpoints']:\n",
    "        test_metrics = evaluate_model(model, test_loader, device)\n",
    "        results['checkpoints'][epoch] = {\n",
    "            'train_loss': epoch_loss / len(train_loader),\n",
    "            'train_rec': epoch_rec / len(train_loader),\n",
    "            'train_kl': epoch_kl / len(train_loader),\n",
    "            **test_metrics,\n",
    "            'model_state': model.state_dict()\n",
    "        }\n",
    "    \n",
    "    # Save results\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    with open(f\"{output_dir}/config_{experement_hash}_{config_id}.pkl\", 'wb') as f:\n",
    "        pickle.dump(results, f)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def evaluate_model(\n",
    "    model: nn.Module,\n",
    "    test_loader: DataLoader,\n",
    "    device: torch.device,\n",
    "    subsample_factor: int = 15,\n",
    ") -> Dict:\n",
    "    \"\"\"Evaluate model on test set with multiple metrics.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss, rec_loss, kl_loss = 0., 0., 0.\n",
    "    all_x, all_encoder_exit, all_decoder_input, all_recon, all_mu = [], [], [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, in test_loader:\n",
    "            # x = x.to(device)\n",
    "            recon, mu, logvar = model(x)\n",
    "            \n",
    "            # Get intermediate activations\n",
    "            # encoder_exit = model.encoder_exit\n",
    "            # decoder_input = model.decoder_input\n",
    "            \n",
    "            # Calculate losses\n",
    "            loss, rl, kl = vae_loss(recon, x, mu, logvar)\n",
    "            total_loss += loss.item()\n",
    "            rec_loss += rl.item()\n",
    "            kl_loss += kl.item()\n",
    "            \n",
    "            # Store for MI calculation\n",
    "            all_x.append(x.cpu().numpy())\n",
    "            all_recon.append(recon.cpu().numpy())\n",
    "            # all_encoder_exit.append(encoder_exit.cpu().numpy())\n",
    "            # all_decoder_input.append(decoder_input.cpu().numpy())\n",
    "            all_mu.append(mu.cpu().numpy())\n",
    "    \n",
    "    # Calculate averages\n",
    "    num_batches = len(test_loader)\n",
    "    metrics = {\n",
    "        'total': total_loss / num_batches,\n",
    "        'rec': rec_loss / num_batches,\n",
    "        'kl': kl_loss / num_batches,\n",
    "    }\n",
    "    \n",
    "    # Concatenate all batches and subsample for speed\n",
    "    all_x = np.vstack(all_x)[::subsample_factor]\n",
    "    all_x = StandardScaler().fit_transform(all_x.reshape(all_x.shape[0], -1).T).T\n",
    "    all_recon = np.vstack(all_recon)[::subsample_factor]\n",
    "    all_recon = StandardScaler().fit_transform(all_recon.reshape(all_recon.shape[0], -1).T).T\n",
    "    # all_encoder_exit = np.vstack(all_encoder_exit)[::subsample_factor]\n",
    "    # all_decoder_input = np.vstack(all_decoder_input)[::subsample_factor]\n",
    "    all_mu = np.vstack(all_mu)[::subsample_factor]\n",
    "    all_mu = StandardScaler().fit_transform(all_mu.reshape(all_mu.shape[0], -1).T).T\n",
    "    \n",
    "    # Compute mutual information metrics\n",
    "    # try:\n",
    "    #     metrics.update({\n",
    "    #         'mi_x_encoder_exit': float(mi(all_x, all_encoder_exit)),\n",
    "    #     })\n",
    "    # except:\n",
    "    #     metrics.update({\n",
    "    #         'mi_x_encoder_exit': None,\n",
    "    #     })\n",
    "    # try:\n",
    "    #     metrics.update({\n",
    "    #         'mi_encoder_exit_decoder_input': float(mi(all_encoder_exit, all_decoder_input)),\n",
    "    #     })\n",
    "    # except:\n",
    "    #     metrics.update({\n",
    "    #         'mi_encoder_exit_decoder_input': None,\n",
    "    #     })\n",
    "    # try:\n",
    "    #     metrics.update({\n",
    "    #         'mi_decoder_input_recon': float(mi(all_decoder_input, all_recon)),\n",
    "    #     })\n",
    "    # except:\n",
    "    #     metrics.update({\n",
    "    #         'mi_decoder_input_recon': None,\n",
    "    #     })\n",
    "    try:\n",
    "        metrics.update({\n",
    "            'mi_x_recon': float(mi(all_x, all_recon)),\n",
    "        })\n",
    "    except:\n",
    "        print(\"mi_x_recon:\", all_x[0], all_recon[0])\n",
    "        metrics.update({\n",
    "            'mi_x_recon': None,\n",
    "        })\n",
    "    try:\n",
    "        metrics.update({\n",
    "            'mi_x_mu': float(mi(all_x, all_mu)),\n",
    "        })\n",
    "    except:\n",
    "        print(\"mi_x_mu:\", all_x[0], all_mu[0])\n",
    "        metrics.update({\n",
    "            'mi_x_mu': None,\n",
    "        })\n",
    "    try:\n",
    "        metrics.update({\n",
    "            'mi_mu_recon': float(mi(all_mu, all_recon)),\n",
    "        })\n",
    "    except:\n",
    "        print(\"mi_mu_recon:\", all_mu[0], all_recon[0])\n",
    "        metrics.update({\n",
    "            'mi_mu_recon': None,\n",
    "        })\n",
    "    try:\n",
    "        metrics.update({\n",
    "            'cca_x_recon': float(canonical_correlation(all_x, all_recon)),\n",
    "        })\n",
    "    except:\n",
    "        print(\"cca_x_recon:\", all_x[0], all_recon[0])\n",
    "        metrics.update({\n",
    "            'cca_x_recon': None,\n",
    "        })\n",
    "    try:\n",
    "        metrics.update({\n",
    "            'cca_x_mu': float(canonical_correlation(all_x, all_mu)),\n",
    "        })\n",
    "    except:\n",
    "        print(\"cca_x_mu:\", all_x[0], all_mu[0])\n",
    "        metrics.update({\n",
    "            'cca_x_mu': None,\n",
    "        })\n",
    "    try:\n",
    "        metrics.update({\n",
    "            'cca_mu_recon': float(canonical_correlation(all_mu, all_recon)),\n",
    "        })\n",
    "    except:\n",
    "        print(\"cca_mu_recon:\", all_mu[0], all_recon[0])\n",
    "        metrics.update({\n",
    "            'cca_mu_recon': None,\n",
    "        })\n",
    "\n",
    "    \n",
    "    # Extract weights for each part of the model\n",
    "    encoder_weights = []\n",
    "    flow_weights = []\n",
    "    decoder_weights = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for name, param in model.named_parameters():\n",
    "            # Get encoder weights (conv layers)\n",
    "            if 'encoder' in name and 'weight' in name:\n",
    "                encoder_weights.append(param.cpu().detach().numpy().flatten())\n",
    "\n",
    "            # Get flow weights (fc layers)\n",
    "            if ('fc_mu' in name or 'fc_logvar' in name or 'fc_decode' in name) and 'weight' in name:\n",
    "                flow_weights.append(param.cpu().detach().numpy().flatten())\n",
    "\n",
    "            # Get decoder weights (conv transpose layers)\n",
    "            if 'decoder' in name and 'weight' in name:  \n",
    "                decoder_weights.append(param.cpu().detach().numpy().flatten())\n",
    "            \n",
    "    \n",
    "    # Concatenate all weights\n",
    "    encoder_weights = np.concatenate(encoder_weights)\n",
    "    flow_weights = np.concatenate(flow_weights)\n",
    "    decoder_weights = np.concatenate(decoder_weights)\n",
    "    \n",
    "    # Compute kurtosis for weights\n",
    "    metrics.update({\n",
    "        'kurtosis_encoder_weights': kurtosis(encoder_weights),\n",
    "        'kurtosis_flow_weights': kurtosis(flow_weights),\n",
    "        'kurtosis_decoder_weights': kurtosis(decoder_weights)\n",
    "    })\n",
    "    \n",
    "    # Compute quantiles for weights [10%, 30%, 50%, 70%, 90%]\n",
    "    quantiles = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "    \n",
    "    metrics.update({\n",
    "        **{f'quantile_{q}_encoder_weights': vq for q, vq in zip(quantiles, np.quantile(np.abs(encoder_weights), quantiles).tolist())},\n",
    "        **{f'quantile_{q}_flow_weights': vq for q, vq in zip(quantiles, np.quantile(np.abs(flow_weights), quantiles).tolist())},\n",
    "        **{f'quantile_{q}_decoder_weights': vq for q, vq in zip(quantiles, np.quantile(np.abs(decoder_weights), quantiles).tolist())}\n",
    "    })\n",
    "    \n",
    "    return {\"test_\" + k: v for k, v in metrics.items()}\n",
    "\n",
    "def calculate_max_num_conv_layers(input_size: int, kernel_size: int, stride: int, padding: int) -> int:\n",
    "    \"\"\"\n",
    "    Вычисляет число слоев после которых раз изображения станет равно 1х1\n",
    "\n",
    "    Args:\n",
    "        input_size (int): Размер входа (N для изображения N x N).\n",
    "        kernel_size (int): Размер ядра свёртки (K).\n",
    "        stride (int): Шаг свёртки (S).\n",
    "        padding (int): Паддинг (P).\n",
    "\n",
    "    Returns:\n",
    "        int: Размер изображения после M свёрток.\n",
    "    \"\"\"\n",
    "    current_size = input_size\n",
    "    prev_size = -1\n",
    "    max_num_layers = 0\n",
    "    while (current_size != prev_size) & (current_size > 1):\n",
    "        prev_size = current_size\n",
    "        max_num_layers += 1\n",
    "        current_size = int(np.floor((prev_size + 2 * padding - kernel_size) / stride + 1))\n",
    "    return max_num_layers - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187c5e67",
   "metadata": {},
   "source": [
    "## CIFAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c69b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_loader, test_loader = get_cifar_dataloader(device=device, train_batch_size=1000, test_batch_size=5000)\n",
    "experement_hash = generate_random_hash(length=16)\n",
    "print(experement_hash)\n",
    "\n",
    "for i in range(0, 1000):\n",
    "    kernel_size = np.random.randint(3, 7)\n",
    "    stride = np.random.randint(1, kernel_size)\n",
    "    if stride == 1:\n",
    "        padding = np.random.randint(0, kernel_size // 2)\n",
    "    else:\n",
    "        padding = np.random.randint(0, kernel_size // 2 + 1)\n",
    "\n",
    "    max_num_layers = min(5, calculate_max_num_conv_layers(input_size = 32, kernel_size = kernel_size, stride = stride, padding = padding))\n",
    "    num_layers = np.random.randint(1, max_num_layers + 1)\n",
    "\n",
    "    latent_dim = 8 * np.random.randint(2, 65)\n",
    "    # enc_filters = sorted([round(1.25 ** np.random.randint(5, 25)) for _ in range(num_layers)])\n",
    "    enc_filters = random.sample(list(range(5, 25)), num_layers)\n",
    "    enc_filters = sorted([round(1.25 ** x) for x in enc_filters])\n",
    "\n",
    "    max_epoch = 100\n",
    "\n",
    "    # Train and evaluate single configuration\n",
    "    try:\n",
    "        results = train_and_evaluate(\n",
    "            config_id=i,\n",
    "            output_dir=\"./results_cifar_AE\",\n",
    "            train_loader = train_loader,\n",
    "            test_loader = test_loader,\n",
    "            latent_dim = latent_dim,\n",
    "            enc_filters = enc_filters,\n",
    "            device = device,\n",
    "            experement_hash = experement_hash,\n",
    "            is_variational=False,\n",
    "            kernel_size = kernel_size,\n",
    "            stride = stride,\n",
    "            padding = padding,\n",
    "            epochs = max_epoch,\n",
    "            input_channels = 3,\n",
    "            input_size = 32,    \n",
    "            checkpoint_epochs = [1, 5]  # Сохранить метрики на 1 и 10 эпохах\n",
    "        )\n",
    "    except TimeoutError:\n",
    "        continue\n",
    "    except ValueError:\n",
    "        continue\n",
    "    except torch.OutOfMemoryError:\n",
    "        print(\"Получившаяся конфигурация переполнила панять GPU!\")\n",
    "        torch.cuda.empty_cache()\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6199c2bb",
   "metadata": {},
   "source": [
    "## MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdee6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_loader, test_loader = get_fashion_mnist_dataloader(device=device, train_batch_size=1000, test_batch_size=5000)\n",
    "experement_hash = generate_random_hash(length=16)\n",
    "print(experement_hash)\n",
    "\n",
    "for i in range(0, 1000):\n",
    "    kernel_size = np.random.randint(3, 6)\n",
    "    stride = np.random.randint(1, kernel_size)\n",
    "    if stride == 1:\n",
    "        padding = np.random.randint(0, kernel_size // 2)\n",
    "    else:\n",
    "        padding = np.random.randint(0, kernel_size // 2 + 1)\n",
    "\n",
    "    max_num_layers = min(5, calculate_max_num_conv_layers(input_size = 28, kernel_size = kernel_size, stride = stride, padding = padding))\n",
    "    num_layers = np.random.randint(1, max_num_layers + 1)\n",
    "\n",
    "    latent_dim = 8 * np.random.randint(2, 65)\n",
    "    # enc_filters = sorted([round(1.25 ** np.random.randint(5, 25)) for _ in range(num_layers)])\n",
    "    enc_filters = random.sample(list(range(5, 25)), num_layers)\n",
    "    enc_filters = sorted([round(1.25 ** x) for x in enc_filters])\n",
    "\n",
    "    max_epoch = 100\n",
    "\n",
    "    # Train and evaluate single configuration\n",
    "    try:\n",
    "        results = train_and_evaluate(\n",
    "            config_id=i,\n",
    "            output_dir=\"./results_fmnist_AE\",\n",
    "            train_loader = train_loader,\n",
    "            test_loader = test_loader,\n",
    "            latent_dim = latent_dim,\n",
    "            enc_filters = enc_filters,\n",
    "            device = device,\n",
    "            experement_hash = experement_hash,\n",
    "            is_variational=False,\n",
    "            kernel_size = kernel_size,\n",
    "            stride = stride,\n",
    "            padding = padding,\n",
    "            epochs = max_epoch,\n",
    "            input_channels = 1,\n",
    "            input_size = 28,    \n",
    "            checkpoint_epochs = [1, 5]  # Сохранить метрики на 1 и 10 эпохах\n",
    "        )\n",
    "    except TimeoutError:\n",
    "        continue\n",
    "    except ValueError:\n",
    "        continue\n",
    "    except torch.OutOfMemoryError:\n",
    "        print(\"Получившаяся конфигурация переполнила память GPU!\")\n",
    "        torch.cuda.empty_cache()\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00082298",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfde308a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b6c556",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f98e1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
